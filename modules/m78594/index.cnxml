<document xmlns="http://cnx.rice.edu/cnxml">
  <title>Introduction to Computer Systems</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m78594</md:content-id>
  <md:title>Introduction to Computer Systems</md:title>
  <md:abstract/>
  <md:uuid>3190159f-ddb6-4b60-94d9-f5ac7b8b081f</md:uuid>
</metadata>

<content>
    <section id="import-auto-idm353434576">
      <title>Introduction to Computer Systems</title>
      <section id="import-auto-idm349713216">
        <title>Overview of the History of the Digital Computer</title>
        <para id="import-auto-idm875674784">Early development of computers centers on mechanical calculators, which are built to perform basic arithmetic operations, such as the Sumerian abacus designed around 2500 BC, and the mechanical calculator, which could perform arithmetic operations invented in the Renaissance. The significance of the mechanical calculator is twofold. First, in trying to develop more powerful and more flexible calculators, Charles Babbage is the first to theorize computers. Second, the design of the mechanical calculator has led to Intel’s development for a low-cost microprocessor, an electronic version of the mechanical calculator, which is the first commercially available microprocessor.  Digital computers are made of electronic components. Vacuum tubes are first used to build computers. However, their huge size and power consumption draw computer engineers’ attention to seek a compact and less power consumption device. Therefore, after the first success of building computers, transistors are used to build computer, followed by large scale integration circuit (LSI), and very larger scale integrated circuit (VLSI).</para>
        <section id="import-auto-idm438830240">
          <title>Vacuum Tubes</title>
          <para id="import-auto-idm353532656">Modern computers started from the development of ENIAC (Electronic Numerical Integrator and Computer) using vacuum tubes, built by John Mauchley and J. Presper Eckert of the university of Pennsylvania, in 1946. ENIAC was originally designed to compute artillery firing tables for the United States Army. It is able to perform arithmetic operations upon ten-digit decimal numbers. It is a modular design with a branch capability based on the sign of a computation result. </para>
          <para id="import-auto-idm580328992">ENIAC was built from vacuum tubes, huge, and consumes a lot of power. It is composted of 17,468 vacuum tubes, 7,200 crystal diodes, 1,500 relays, 70,000 resistors, 10,000 capacitors and around 5 million hand-soldered joints. It takes up 1800 square feet, and consumes 150 kW of power.  Input/output may be from an IBM card reader, and an IBM card punch.</para>
          <para id="import-auto-idm881754336">ENIAC could perform 5,000 simple addition or subtraction operations per second, about 385 multiplication operations per second, 40 division operations per second, and 3 square root operations per second. Programs may include sequential arithmetic operations, loops, and subroutines. However, programming takes weeks.  First a problem in question is mapped to a program on paper.  Second, the program is transferred into ENIAC via its switches, and cables. Finally, the program is debugged by single step execution on ENIAC. The lack of the ability to store programs in the machine has led to the development of John von Neumann machines by John von Neumann. </para>
        </section>
        <section id="import-auto-idm571092304">
          <title>Stored Program Computer</title>
          <para id="import-auto-idm844737344">Before the ENIAC was fully functional, John Mauchly and J. Presper Eckert had started working on the project, Electronic Discrete Variable Automatic Computer (EDVAC), which was binary rather than decimal, and was a stored program computer. John von Neumann participated in that project as a consultant. In 1945, John von Neumann summarized and elaborated the logic design and development of EDVAC in his article entitled “First Draft of a Report on the EDVAC” which was known as Von Neumann architecture (a.k.a. Von Neumann model). In this architecture, a computer system is composed of a process unit, which contains an arithmetic logic unit (ALU), registers, a control unit containing an instructor register, a program counter, a memory that stores both instructions and data, external mass storage, and I/O mechanisms. The memory sharing of instructions and data causes the instruction fetch and data fetch not to occur at the same time, resulting in a performance limit. The modern stored-program architecture known as Harvard architecture enhances the performance by dedicating one bus for instructions and another one for data.</para>
          <para id="import-auto-idm685901344">Another branch of the stored program architecture research was conducted by Soviet scientists Sergei Sobolev and Nikolay Brusentsov on ternary computers, which were operated on a base three numerical system of −1, 0, and 1 rather than the binary numerical system, upon which most contemporary computers are based. This computer, however, was turned to the binary numerical system later.</para>
        </section>
        <section id="import-auto-idm460623712">
          <title>Transistor</title>
          <para id="import-auto-idm884991952">Vacuum tubes were used to build computers in 1950s. With the advent in electronic components, transistors replaced vacuum tubes in building electronic devices including computers in 1960s. Transistors are cheaper, faster, less power, and more reliable than vacuum tubes. The first transistor computer was demonstrated  by the University of Manchester in 1953. That machine was composed of 550 diodes, and 92 point-contact transistors, the first type of solid-state electronic transistor constructed by researchers John Bardeen and Walter Houser Brattain at Bell Laboratories. Each word in the machine is of length 48 bits. Its clock generator was constructed of a few vacuum tubes. </para>
        </section>
        <section id="import-auto-idm878060784">
          <title>Integrated Circuit</title>
          <para id="import-auto-idm349837984">In the 1970s, integrated circuit (IC) technology had further reduced the size and cost of computers with an increasing computation speed. Microcontrollers were vastly used in appliances such as video recorders, laundry machines. Robert Noyce invented the silicon integrated circuit in 1958. Semiconductor devices are observed to perform similar functions as vacuum tubes. The advancement in device fabrication allows the packing of a large number of transistors in a small chip, which would no longer require assembling discrete electronic components manually. The mass production of ICs results in reliable, modularized, low cost produces, in which blocks of transistors in the circuit are replaced with corresponding ICs. It low cost is due to the chips with their components are printed as a unit by photolithography rather than being produced one transistor at a time. Substance used in IC is less in terms of area per unit transistor. The performance of IC is better because it consumes less power and switches quickly for the closer components within. In this era, IBM’s manufactured two highly successful machines such as the 7094 and 1401, which were replaced by IBM System/360 Model 75 and Model 30 later, respectively. Intel’s MCS-4 family such as 4004, was a 4-bit CPU released in 1971. It was the first commercially available complete CPU on a chip. The 4004 CPU is built of approximately 2,300 transistors, running up to 740 KHz, 46 instructions (8/16-bit wide), 16 4-bit registers, 3-level deep subroutine stacks, and 12-bit address.</para>
        </section>
        <section id="import-auto-idm355265328">
          <title>Very Large Scale Integrated Circuit</title>
          <para id="import-auto-idm355344144">In the 1980s, further advancement in integrated circuit and packaging allowed putting millions of transistors on a single chip, known as very large scale integrated circuit (VLSI). This technology made it feasible to built fairly low cost and small size personal computers. The early two major commercially available personal computers are Intel’s 8080 and Apple II. The Intel’s 8080 computer comes with parts including the 8080 CPU, cables, power supply, 8” floppy disk, but without any software. The CP/M operating system, written by Gary Kildall, was prevalently running on 8080s. Apple II was designed by Steve Jobs and Steve Wozniak in their garage. The machine comes with a built-in Basic interpreter, which makes programming easier and fun. The machine is popular with home users and students at schools. </para>
          <para id="import-auto-idm352295184">Table 1 Milestones in Computer Development</para>
          
            <media id="import-auto-idm894621008" alt="">
              <image mime-type="image/png" src="../../media/Image1-97d7.png" height="665" width="624"/>
            </media>
          
        </section>
      </section>
      <section id="import-auto-idm718719360">
        <title>Introduction to Instruction Set Architecture, Microarchitecture and System Architecture</title>
        <section id="import-auto-idm664325568">
          <title>Instruction Set Architecture</title>
          <para id="import-auto-idm327839696">An instruction set architecture (ISA) details programming models and computing abstraction for a processor. Assembly programmers and compiler writers create programs by consulting an ISA. An instruction set of a CPU dictates what a CPU can do and what it can do efficiently. At the first design stage of a CPU, an instruction set has to be developed. Instruction set architecture is one of the most important design dimensions that a CPU designer must get right since its onset. A good instruction set architecture would benefit other features such as code size, instruction encoding/decoding, pipelining, caches, superscalar, and the like. A typical instruction should contain information such as op-code, operands, addressing modes, immediate values, etc. Op-code instructs the CPU to perform the operation such as addition, logical AND, etc. Operands indicate the values to be computed upon, including source operands and a target operand, where the computation result will be stored. Addressing modes specify where to obtain the operands. Normally, operands are coming from registers or memory. Immediate values are operands coming along with instruction stream. Small constants, e.g., are often encoded in the instruction to eliminate extra operands fetches.  depicts a typical instruction.  Note that the Src2 and Dst operands may be combined into one field. In that case, the computation would become <media id="import-auto-idm350001040" alt=""><image mime-type="image/png" src="../../media/Image2-22ad.png"/></media> instead of <media id="import-auto-idm316629952" alt=""><image mime-type="image/png" src="../../media/Image3-909a.png"/></media>. Moreover, the immediate value may span across several fields to accommodate a larger value. </para>
          <para id="import-auto-idm355288448">Table 2 A Typical Instruction</para>

            <media id="import-auto-idm359224080" alt="">
              <image mime-type="image/png" src="../../media/Image4-2037.png"/>
            </media>

          <para id="import-auto-idm589458400">It is nearly impossible to implement everything in a CPU due to the space limit. Each instruction takes some silicon real estate in terms of the number of transistors. The Intel 4004 CPU uses about 2,300 transistors whereas Pentium 4 uses 42,000,000 transistors. Although the budget of transistors for multi-core processors is huge, e.g., 10-core Xeon Westmere-EX employs 2,600,000,000 transistors,  each added feature demands quite a few of transistors. Thus, only the instruction that is a must is added to the instruction set. A good example goes to the subtraction instruction. Since subtraction can be translated to addition with two’s complement, it is not necessary to include a subtraction instruction. Instead, a two’s complement unit may be implemented along with an addition instruction to achieve subtraction. For example, <media id="import-auto-idm356274896" alt=""><image mime-type="image/png" src="../../media/Image5-6173.png"/></media> where <media id="import-auto-idm884337184" alt=""><image mime-type="image/png" src="../../media/Image6.png"/></media> is the two’s complement of  <media id="import-auto-idm716258064" alt=""><image mime-type="image/png" src="../../media/Image7.png"/></media>. </para>
          <para id="import-auto-idm640950240">The rule of thumb in hardware design is the less, the better. The less resource used, the more benefits. The benefits include 1) the cost is low, 2) the circuit is simple, 3) the power consumption is low, and 4) the verification is easy. A complicate instruction set would demand a lot of space, and the cost would be really high. In implementing a complex instruction set, the circuit would not be simple, and nor is the verification of the correctness of the CPU. A huge amount of transistors also means that power consumption will be fairly high. </para>
          <para id="import-auto-idm341789120">One of the difficult design concerns for an instruction set architecture is that what instructions have to be included. It would be really hard to predict ten years down the road what instructions are for popular applications. Nevertheless, most of the CPUs over the past may only exist for only a couple of year.  For example, Intel’s MMX extension to its Pentium CPUs reflects the need for multimedia applications. Nobody would ever know that MMX should be included in Intel’s 8080 processors. </para>
          <para id="import-auto-idm354468688">Adding instruction to CPU is easier than taking it out for a number of reasons. First, there is a backward compatible issue. A lot of applications are based on the instruction set. Taking instructions out from the instruction set means breaking out their executions. On the other hand, adding in new instructions would not affect their executions because only the old set of instructions is used. This is called backward compatibility. Compatibility backward is crucial in computer industry. Even with the latest Windows 7, there is still a DOS windows that may execute a DOS program developed in ‘80s. </para>
          <para id="import-auto-idm351042384">An instruction set should be designed in a way that most of assembler or compiler writers would develop system programs easily. The system programs will help application programmers develop programs that run on the processors. The popularity of a process lies on its complexity. Simple wins all. A complex system would never be popular. </para>
        </section>
        <section id="import-auto-idm1567401568">
          <title>Microarchitecture</title>
          <para id="import-auto-idm610106944">The way a given instruction set architecture (ISA) is implemented on a processor is called microarchitecture, also called computer organization. Computer architecture is the combination of the ISA and the microarchitecture. A microarchitecture is typically represented by a diagram that depicts the interconnections among microarchitectural components, which include gates, function units, registers, arithmetic logic unit (ALU), and the like. The diagram includes the data path, where data flows through components, and the control path, where coordination of data flows is under control. A schematic diagram is used to describe a microarchitecture element in gates level. Each gate is represented by transistors with their interconnections for a specific logic device. A given ISA may be implemented using different microarchitectures. For example, the x86 ISA has been implemented in processors manufactured by Intel, AMD, and Via.  The latest VLSI technology may improve the performance of processors using an old ISA.</para>
        </section>
        <section id="import-auto-idm354573040">
          <title>System Architecture</title>
          <para id="import-auto-idm429339664">A system architecture is a formal abstraction model that describes the system components, structure, behavior, and relations among components.  In a computer system that includes hardware and software, a system architect is concerns with the complete artifact, both hardware and software, and all the interfaces of the artifact including those among software components, among hardware components, between hardware and software, and between the artifact and its users. A good system architecture design will include requirements of the overall system. The requirements are mapped to a set of system components (hardware or software), which may be validated against the requirement via a series of system tests. To some extent, the system architecture specifies what have to done and what to expect, other than how to get things done. Hardware engineers and software engineers, on the other hand, design hardware/software components according to the high level system architecture. Instruction set architecture, and microarchitecture are then developed. Software engineers develop each software components running on the hardware according to the system architecture. Each system component is verified and tested by corresponding designers. The overall system after integrating system components is then under a series of thorough system tests to guarantee the system requirements are met. </para>
        </section>
      </section>
      <section id="import-auto-idm1560985536">
        <title>Processor Architecture – Instruction Types, Register Sets, Addressing Modes</title>
        <para id="import-auto-idm626205536">A typical process is composed of registers, ALU, instruction decoding unit, and control units. There are special registers to keep CPU working such as program counter (PC), stack pointer (SP), status register (SR)<sup><footnote id="import-auto-footnote-1"> Some processor otherwise use the name program status word (PSW).</footnote></sup>. PC is holding the address of next instruction to be executed. SP is pointing to the top of the stack in memory. SR is a register that keeps the process state after each instruction has been executed. An instruction register, one of the registers, is used to keep the instruction currently under execution. The instruction register is connected to the instruction decoding unit, where the outputs are control signals that synchronize all the components inside CPU. </para>
        <section id="import-auto-idm757510224">
          <title>Instruction Types</title>
          <para id="import-auto-idm615130464">There are various classes of instruction: data movement, arithmetic, bit-wise, and flow control. Data movement instructions transfer data from registers to registers, from registers to memory, from memory to registers, and from memory to memory (for some CPUs). Data movement is to prepare operands for the further computation, or simply to store data in memory when computation is completed.</para>
          <para id="import-auto-idm352381712">Arithmetic instructions include arithmetic operations such as addition, subtraction, multiplication, division, and the like. Some processors provide the complicated operations such as multiplication if space is enough. In processors only supporting basic arithmetic operations, the complex instruction, e.g., multiplication, would have to be emulated by either subroutines or macros, meaning that a software approach is needed to achieve the complex operations. This is a tradeoff between performance and cost (space). With the hardware complex instructions, the performance is high but a significant amount of hardware resource is required. </para>
          <para id="import-auto-idm439145104">Bit-wise operations contain AND, OR, NOT, XOR, and the like. The bit-wise operations work on each individual bit of data, and sometimes, may not lead to the same result as their corresponding logical operations. For example, <media id="import-auto-idm669899152" alt=""><image mime-type="image/png" src="../../media/Image8.png"/></media> because <media id="import-auto-idm349850064" alt=""><image mime-type="image/png" src="../../media/Image9.png"/></media>. If the logical false is defined as 0, and a non-zero value is considered as logical true, the logical AND operates upon 0x1 and 0x2 should be true because <media id="import-auto-idm349491712" alt=""><image mime-type="image/png" src="../../media/Image10.png"/></media>. Practically, we may still use bit-wise operations for their corresponding logical operations if the logical true is limited to be 0x1. In that case, the logical operations will yield the same results as the bit-wise operations. Some special operations such as “remainder of dividing by 8” may be accomplished by the logical operations. For example, the remainder of a number <media id="import-auto-idm351980464" alt=""><image mime-type="image/png" src="../../media/Image11.png"/></media> divided by 16 can be computed by the following statement:</para>
          
            <media id="import-auto-idm695033888" alt="">
              <image mime-type="image/png" src="../../media/Image12.png"/>
            </media>

          <para id="import-auto-idm442165152">The logical AND is a bit-wise operation, and the result of the above statement is actually taking the lower 4 –bits from the number x, which is exactly the remainder!</para>
          <para id="import-auto-idm844674256">Flow control instructions are composed of jumps, predicates, and subroutines which are required to implement program structures such as if-then-else, case-switch, loops, subroutines, etc. Jump instructions include unconditional jumps and conditional jumps. Unconditional jumps implements go-to statements in a loop or a conditional if structure. Conditional jumps involve a predicate instruction, which evaluates a Boolean expression or a relational expression. The Boolean expression or relational expression will be evaluated to either true or false. Therefore, before a conditional instruction, there is always a predicate instruction such as comparison of two numbers. The result either true or false is then used for the conditional jump. The end result of a jump instruction is the change of the program counter (PC). Other instructions that modify PC include subroutine calls. When a subroutine call is made, PC will be loaded with the beginning address of the subroutine. The program control is then transferred to the subroutine. By the time the subroutine finishes it execution, the PC will be loaded with the address of next instruction followed by the subroutine call. </para>
        </section>
        <section id="import-auto-idm618232128">
          <title>Addressing Modes</title>
          <para id="import-auto-idm877464000">Addressing modes of a processor dictate where to load the operands for an instruction. The operands may be from registers, memory, or the instruction itself. Typical addressing modes include register direct, direct, register indirect, index, and immediate. Not all processors implement all addressing modes because again there is a tradeoff between cost and performance. The more addressing mode supported, the more space and cost will be. However, almost all processers support register direct addressing mode. In register direct, operands are stored in register. Direct addressing allows the address of operands to be specified in the instruction. Register indirect addressing mode stored the address of an operand in a register. It is typically used to specify a memory operand. Index addressing mode is also used to specify a memory operand, especially an array element, with the form, <media id="import-auto-idm612459152" alt=""><image mime-type="image/png" src="../../media/Image13.png"/></media> where base is the starting address and <media id="import-auto-idm352555680" alt=""><image mime-type="image/png" src="../../media/Image14.png"/></media> is a register that keeps index. The effective address is calculated by <media id="import-auto-idm308888560" alt=""><image mime-type="image/png" src="../../media/Image15.png"/></media>. Since the array element is of the same size, access array element is simply changing the index register R. There are other special addressing modes, e.g., symbolic addressing and absolute addressing, to be discussed in a later section.</para>
        </section>
      </section>
      <section id="import-auto-idm659879600">
        <title>Processor Structures – Memory-to-Register and Load/Store Architectures</title>
        <para id="import-auto-idm351921264">The term “Load” is an operation that moves data from memory to registers whereas the term “Store” is referring to the opposite operations, i.e., store data in memory from registers. In most reduced instruction set computer (RISC) architecture, such as MIPS, there are abundant of registers, and memory accesses are 100-400 times slower than registers. Thus, the intent is to keep operands in registers for operations, and limit only the load and store instructions to interact with memory. This type of architecture is normally called load/store architecture. </para>
        <para id="import-auto-idm755341344">Data movement in memory-to-register architecture is determined by addressing modes. In this architecture, every instruction (not just load/store instructions in the load/store architecture) could move data around potentially. For example, in MSP430, the instruction ADD 0(R4), R5 will sum R5 and the content at the memory address<media id="import-auto-idm622459152" alt=""> <image mime-type="image/png" src="../../media/Image16.png"/></media>, and put the result in the register <media id="import-auto-idm355727792" alt=""><image mime-type="image/png" src="../../media/Image17.png"/></media>. This instruction actually moves data from memory to register implicitly. On the other hand, the instruction ADD 0(R4), 0(R5) will move data from memory to registers, operate upon them, and move the result back to memory. The two source operands are specified in memory at locations, <media id="import-auto-idm343793984" alt=""><image mime-type="image/png" src="../../media/Image18.png"/></media>. The results is stored in the memory at location <media id="import-auto-idm583280656" alt=""><image mime-type="image/png" src="../../media/Image19.png"/></media>. </para>
        <para id="import-auto-idm316691456">The difference between memory-to-register and load/store architectures is that only load and store instructions may access memory in the load/store architecture but all instructions with suitable addressing modes are allowed to access memory in the memory-to-register architecture. The memory-to-register architecture is typically implemented in complex instruction set computers (CISC), which provide flexibility with a more complex process design. </para>
      </section>
      <section id="import-auto-idm313682656">
        <title>Instruction Sequencing, Flow-of-Control, Subroutine Call and Return Mechanisms</title>
        <para id="import-auto-idm601156720">Instructions in a program are executed in sequentially on a single processor system. This instruction sequencing is broken when there is a need such as a condition change or an event occurs. For example, a program that counts how many A’s and how many non-A’s in a grading sheet. The program logic should depend on the grade input. If the grade input is  “A”, a piece of code that increases a counter by one for A’s is executed. Otherwise, another piece of code that increases a counter by one for non-As is executed. This program flow is controlled by the data input. </para>
        <para id="import-auto-idm601980064">Conditional operations such as if-then-else and loop structures are implemented and supported at the machine level. There are instructions to support subroutine calls and returns as well as interrupt service routines. These control instructions have to be implemented in machine level, meaning that there are corresponding supporting instructions such as JUMP, CALL, RET, and RETI, where RET is used for the return of a normal subroutine call, and RETI is used for the return from an interrupt service routine.  </para>
      </section>
      <section id="import-auto-idm669686800">
        <title>Structure of Machine-Level Programs</title>
        <para id="import-auto-idm714339776">Most of the machine-level programs such as assembly programs are very organized. They are written in mnemonic symbols, each of which has a corresponding machine instruction. For example, ADD is used for addition, SUB is used for subtraction, etc. A label is used to represent a memory location. Most of the assembly programming follow a four-column format as illustrated in .</para>
        <para id="import-auto-idm1553293600">Table 3 The Four-Column Format in Assembly Programming</para>

          <media id="import-auto-idm463204416" alt="">
            <image mime-type="image/png" src="../../media/Image20.png"/>
          </media>

        <para id="import-auto-idm356544176">The first column specifies a label, which designates a particular program memory address in question. Normally, this address would be the target for a jump or the beginning of a loop or a subroutine. The second column specifies an operation via its mnemonic symbol, which typically is called op-code. The third column declares operands. There may have several operands subject to a processor design. The order of the operands determines which of them are sources, and which of them is the destination. The fourth column is the comments, preceded by a semicolon normally.  The comments describe what the instruction at that line is all about. It is highly recommended to document as detail as possible as assembly programming is less structural. Comments will greatly increase its readability.  </para>
      </section>
      <section id="import-auto-idm451867312">
        <title>Limitations of Low-Level Architectures</title>
        <para id="import-auto-idm714080624">The instruction set architecture specifies what a process can do. A program to be executed on a processor would have to contain instructions from its instruction set. Obviously, there are a number of operations that are not implemented directly in its ISA. For example, multiplication is not normally implemented in embedded processors such as MSP430. In order for the missing operations to be still performed on a processor which may not directly implemented the missing operation, a software approach such as macro or subroutines will be applied. In the multiplication example, a shift-add algorithm may be implemented using all the instructions supported by MSP430. The limit of an architecture, however, varies significantly. Factors resulting in the limit include application, cost, performance of a processor. </para>
        <para id="import-auto-idm314277648">Other architectural limitations include addressing modes, instruction weights, size of registers, etc. Addressing modes are directly supported by the hardware. For example, register indirect addressing mode is considered as a high performance mechanism to access an operand in memory in MSP430. However, it only supports source operand. The destination operand may not be specified by register indirect addressing. A workaround would be index addressing, but it is slower than register indirect addressing. </para>
        <para id="import-auto-idm582922960">Not all instructions are created equal. Some instructions may require more clock cycles to finish. Some may require less. The number of registers is fixed in a processor. Some processor has more registers but some has less. Knowing the limitations of low-level architectures is quite important if a system is developed not just to be correct but also high performance.  </para>
      </section>
      <section id="import-auto-idm359243776">
        <title>Low-Level Architectural Support for High-Level Languages</title>
        <para id="import-auto-idm311592640">The higher level programs have to be translated to machine code eventually. Should there is no support at a particular ISA, the design of compiler along with its performance will be affected drastically. The low-level architecture supports such as user-defined subroutines and interrupt service routines directly affect the design and the performance of high-level languages.</para>
        <para id="import-auto-idm888225280">The high-level program constructs such as if-then-else’s, do-loop’s, functions, and interrupt service routines are translated to suitable machine instructions. This translation is normally done by a compiler. A compiler writer, therefore, would have to figure out what machine instructions may be used to support those high-level programming constructs. </para>
        <para id="import-auto-idm461515824">The syntax for high-level programming languages is as follows:</para>
        <para id="import-auto-idm719806256">Table 5 Syntax of It-Then-Else Program Construct</para>
<code id="syntax-ifelse" display="block">
if B then
  S1;
else
  S2;
end if;
</code>

        <para id="import-auto-idm582257280">In , B is a Boolean expression, and S1 and S2 are statements. The Boolean expression will be evaluated to either true or false. That condition is then used to change program control flow. This program construct when translated to machine code will have the following structure:</para>
        <para id="import-auto-idm338919104">Table 6 Machine Code Structure for an If-Then-Else Construct</para>
<code id="machinecode-ifelse" display="block">
	B		; code for B
	JF False	; jump if B is false
	S1		; true statement
	J EndIf		; jump to EndIf
False:	S2		; false statement
EndIf:			; end if
</code>

        <para id="import-auto-idm327809152">If B, S1, and S2 can be translated using supported instructions, the low-level instructions JF and J would have to be supported. Here JF is a conditional jump instruction subject to the result of its previous instruction. Most processors will keep execution results in a status register (SR). The JF instruction will check SR before change the control flow. Another instruction J, unconditional jump, often is supported. A loop has a similar structure. </para>
        <para id="import-auto-idm303103344">At the assembly language level, how parameters are passed to subroutines and how local workplace is created and accessed will definitely affect the overall performance. The lack of resources has an impact on high-level languages and the design of compilers. An argument stack is typically used to pass actual parameters to a function in most high-level languages. Access to each of the parameters will rely on stack operations, such as PUSH and POP. The hardware support for the stack operations is essential to the system performance in light of the heavy use of subroutines and functions in high-level programming. </para>
        <para id="import-auto-idm484950528">Two most important instructions that hardware must support in order to execute subroutines or functions are CALL and RET. The CALL instruction will change the value of PC to the starting address of the subroutine to be called, and push the address (i.e., return address) of next instruction after the call instruction itself in the stack. Although the programmer may freely change PC, the address of next instruction may not be known until runtime. Therefore, the hardware support for the CALL instruction is a must. The RET instruction is the last instruction in a subroutine or a function. Its purpose is to rewind stack and restore PC with the stored return address. Subject to the CPU design, the RET instruction may also perform other tasks such as restoring SR.  </para>
      </section>
    </section>
  </content>
</document>